{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e782a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k=1 and fold 1 as test, with feature normalization\n",
      "Accuracy: %99.732292\n",
      "Precision: %97.858117\n",
      "Recall: %97.853506\n",
      "\n",
      "For k=1 and fold 1 as test, without feature normalization\n",
      "Accuracy: %99.731250\n",
      "Precision: %97.848693\n",
      "Recall: %97.844731\n",
      "\n",
      "For k=1 and fold 2 as test, with feature normalization\n",
      "Accuracy: %99.703125\n",
      "Precision: %97.627219\n",
      "Recall: %97.623477\n",
      "\n",
      "For k=1 and fold 2 as test, without feature normalization\n",
      "Accuracy: %99.712500\n",
      "Precision: %97.700491\n",
      "Recall: %97.699203\n",
      "\n",
      "For k=1 and fold 3 as test, with feature normalization\n",
      "Accuracy: %99.708333\n",
      "Precision: %97.673275\n",
      "Recall: %97.670665\n",
      "\n",
      "For k=1 and fold 3 as test, without feature normalization\n",
      "Accuracy: %99.709375\n",
      "Precision: %97.682650\n",
      "Recall: %97.680257\n",
      "\n",
      "For k=1 and fold 4 as test, with feature normalization\n",
      "Accuracy: %99.734375\n",
      "Precision: %97.881460\n",
      "Recall: %97.875732\n",
      "\n",
      "For k=1 and fold 4 as test, without feature normalization\n",
      "Accuracy: %99.738542\n",
      "Precision: %97.913647\n",
      "Recall: %97.909411\n",
      "\n",
      "For k=1 and fold 5 as test, with feature normalization\n",
      "Accuracy: %99.720833\n",
      "Precision: %97.763466\n",
      "Recall: %97.761948\n",
      "\n",
      "For k=1 and fold 5 as test, without feature normalization\n",
      "Accuracy: %99.719792\n",
      "Precision: %97.755128\n",
      "Recall: %97.753601\n",
      "\n",
      "For k=3 and fold 1 as test, with feature normalization\n",
      "Accuracy: %99.857292\n",
      "Precision: %98.864493\n",
      "Recall: %98.854758\n",
      "\n",
      "For k=3 and fold 1 as test, without feature normalization\n",
      "Accuracy: %99.861458\n",
      "Precision: %98.897845\n",
      "Recall: %98.887869\n",
      "\n",
      "For k=3 and fold 2 as test, with feature normalization\n",
      "Accuracy: %99.860417\n",
      "Precision: %98.880967\n",
      "Recall: %98.884421\n",
      "\n",
      "For k=3 and fold 2 as test, without feature normalization\n",
      "Accuracy: %99.860417\n",
      "Precision: %98.880209\n",
      "Recall: %98.884522\n",
      "\n",
      "For k=3 and fold 3 as test, with feature normalization\n",
      "Accuracy: %99.863542\n",
      "Precision: %98.912372\n",
      "Recall: %98.912287\n",
      "\n",
      "For k=3 and fold 3 as test, without feature normalization\n",
      "Accuracy: %99.863542\n",
      "Precision: %98.912713\n",
      "Recall: %98.911830\n",
      "\n",
      "For k=3 and fold 4 as test, with feature normalization\n",
      "Accuracy: %99.846875\n",
      "Precision: %98.777899\n",
      "Recall: %98.773858\n",
      "\n",
      "For k=3 and fold 4 as test, without feature normalization\n",
      "Accuracy: %99.846875\n",
      "Precision: %98.778736\n",
      "Recall: %98.774198\n",
      "\n",
      "For k=3 and fold 5 as test, with feature normalization\n",
      "Accuracy: %99.854167\n",
      "Precision: %98.832253\n",
      "Recall: %98.830195\n",
      "\n",
      "For k=3 and fold 5 as test, without feature normalization\n",
      "Accuracy: %99.853125\n",
      "Precision: %98.824162\n",
      "Recall: %98.821438\n",
      "\n",
      "For k=5 and fold 1 as test, with feature normalization\n",
      "Accuracy: %99.867708\n",
      "Precision: %98.945709\n",
      "Recall: %98.937960\n",
      "\n",
      "For k=5 and fold 1 as test, without feature normalization\n",
      "Accuracy: %99.866667\n",
      "Precision: %98.937540\n",
      "Recall: %98.929456\n",
      "\n",
      "For k=5 and fold 2 as test, with feature normalization\n",
      "Accuracy: %99.863542\n",
      "Precision: %98.906750\n",
      "Recall: %98.907845\n",
      "\n",
      "For k=5 and fold 2 as test, without feature normalization\n",
      "Accuracy: %99.862500\n",
      "Precision: %98.898911\n",
      "Recall: %98.899556\n",
      "\n",
      "For k=5 and fold 3 as test, with feature normalization\n",
      "Accuracy: %99.866667\n",
      "Precision: %98.936834\n",
      "Recall: %98.937419\n",
      "\n",
      "For k=5 and fold 3 as test, without feature normalization\n",
      "Accuracy: %99.866667\n",
      "Precision: %98.937229\n",
      "Recall: %98.937050\n",
      "\n",
      "For k=5 and fold 4 as test, with feature normalization\n",
      "Accuracy: %99.858333\n",
      "Precision: %98.869652\n",
      "Recall: %98.865221\n",
      "\n",
      "For k=5 and fold 4 as test, without feature normalization\n",
      "Accuracy: %99.858333\n",
      "Precision: %98.870253\n",
      "Recall: %98.865141\n",
      "\n",
      "For k=5 and fold 5 as test, with feature normalization\n",
      "Accuracy: %99.860417\n",
      "Precision: %98.882784\n",
      "Recall: %98.881258\n",
      "\n",
      "For k=5 and fold 5 as test, without feature normalization\n",
      "Accuracy: %99.860417\n",
      "Precision: %98.882784\n",
      "Recall: %98.881258\n",
      "\n",
      "For k=7 and fold 1 as test, with feature normalization\n",
      "Accuracy: %99.871875\n",
      "Precision: %98.978261\n",
      "Recall: %98.971525\n",
      "\n",
      "For k=7 and fold 1 as test, without feature normalization\n",
      "Accuracy: %99.872917\n",
      "Precision: %98.986787\n",
      "Recall: %98.980040\n",
      "\n",
      "For k=7 and fold 2 as test, with feature normalization\n",
      "Accuracy: %99.868750\n",
      "Precision: %98.947792\n",
      "Recall: %98.950495\n",
      "\n",
      "For k=7 and fold 2 as test, without feature normalization\n",
      "Accuracy: %99.864583\n",
      "Precision: %98.915542\n",
      "Recall: %98.915986\n",
      "\n",
      "For k=7 and fold 3 as test, with feature normalization\n",
      "Accuracy: %99.870833\n",
      "Precision: %98.969713\n",
      "Recall: %98.970550\n",
      "\n",
      "For k=7 and fold 3 as test, without feature normalization\n",
      "Accuracy: %99.870833\n",
      "Precision: %98.969713\n",
      "Recall: %98.970550\n",
      "\n",
      "For k=7 and fold 4 as test, with feature normalization\n",
      "Accuracy: %99.860417\n",
      "Precision: %98.885241\n",
      "Recall: %98.882260\n",
      "\n",
      "For k=7 and fold 4 as test, without feature normalization\n",
      "Accuracy: %99.860417\n",
      "Precision: %98.885241\n",
      "Recall: %98.882260\n",
      "\n",
      "For k=7 and fold 5 as test, with feature normalization\n",
      "Accuracy: %99.863542\n",
      "Precision: %98.907630\n",
      "Recall: %98.906579\n",
      "\n",
      "For k=7 and fold 5 as test, without feature normalization\n",
      "Accuracy: %99.862500\n",
      "Precision: %98.899212\n",
      "Recall: %98.897769\n",
      "\n",
      "For k=9 and fold 1 as test, with feature normalization\n",
      "Accuracy: %99.870833\n",
      "Precision: %98.969756\n",
      "Recall: %98.962951\n",
      "\n",
      "For k=9 and fold 1 as test, without feature normalization\n",
      "Accuracy: %99.870833\n",
      "Precision: %98.969801\n",
      "Recall: %98.962940\n",
      "\n",
      "For k=9 and fold 2 as test, with feature normalization\n",
      "Accuracy: %99.868750\n",
      "Precision: %98.947839\n",
      "Recall: %98.949336\n",
      "\n",
      "For k=9 and fold 2 as test, without feature normalization\n",
      "Accuracy: %99.869792\n",
      "Precision: %98.955872\n",
      "Recall: %98.958678\n",
      "\n",
      "For k=9 and fold 3 as test, with feature normalization\n",
      "Accuracy: %99.872917\n",
      "Precision: %98.985939\n",
      "Recall: %98.988149\n",
      "\n",
      "For k=9 and fold 3 as test, without feature normalization\n",
      "Accuracy: %99.872917\n",
      "Precision: %98.985997\n",
      "Recall: %98.987855\n",
      "\n",
      "For k=9 and fold 4 as test, with feature normalization\n",
      "Accuracy: %99.859375\n",
      "Precision: %98.876526\n",
      "Recall: %98.873926\n",
      "\n",
      "For k=9 and fold 4 as test, without feature normalization\n",
      "Accuracy: %99.859375\n",
      "Precision: %98.876526\n",
      "Recall: %98.873926\n",
      "\n",
      "For k=9 and fold 5 as test, with feature normalization\n",
      "Accuracy: %99.864583\n",
      "Precision: %98.916256\n",
      "Recall: %98.915166\n",
      "\n",
      "For k=9 and fold 5 as test, without feature normalization\n",
      "Accuracy: %99.863542\n",
      "Precision: %98.907543\n",
      "Recall: %98.906743\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# KNN Algorithm\n",
    "\n",
    "\n",
    "def knn(xtrain, ytrain, xtest, k):\n",
    "    distance = np.sum(xtest**2, axis=1)[:, np.newaxis] + np.sum(xtrain**2, axis=1) - 2*np.dot(xtest, xtrain.T)\n",
    "    near_idx = np.argsort(distance, axis=1)[:, :k]\n",
    "    near_classes = ytrain[near_idx]\n",
    "    most_voted_class = [Counter(i).most_common(1)[0][0] for i in near_classes]\n",
    "    pred_arr = np.array(most_voted_class)\n",
    "    return pred_arr\n",
    "\n",
    "# Calculate performance function\n",
    "\n",
    "\n",
    "def performance(pred_arr, ytest):\n",
    "    y_actual_series = pd.Series(ytest, name=\"Actual\")\n",
    "    y_pred_series = pd.Series(pred_arr, name=\"Predicted\")\n",
    "    df_cm = pd.crosstab(y_actual_series, y_pred_series)  # Confusion matrix as DataFrame\n",
    "    cm_arr = np.array(df_cm)  # Confusion matrix as array\n",
    "\n",
    "    tp = np.diag(cm_arr)  # True positive\n",
    "    fn = np.sum(cm_arr, axis=1) - tp  # False negative\n",
    "    fp = np.sum(cm_arr, axis=0) - tp  # False positive\n",
    "    tn = cm_arr.sum() - (tp + fp + fn)  # True negative\n",
    "\n",
    "    accuracies = (tp+tn)/(tp+tn+fp+fn)\n",
    "    precisions = tp/(tp+fp)\n",
    "    recalls = tp/(tp+fn)\n",
    "\n",
    "    macro_accuracy = np.sum(accuracies)/len(accuracies)\n",
    "    macro_precision = np.sum(precisions)/len(precisions)\n",
    "    macro_recall = np.sum(recalls)/len(precisions)\n",
    "\n",
    "    print(\"Accuracy: %%%f\" % (macro_accuracy*100))\n",
    "    print(\"Precision: %%%f\" % (macro_precision*100))\n",
    "    print(\"Recall: %%%f\\n\" % (macro_recall*100))\n",
    "\n",
    "\n",
    "p_dict = {\"ESTJ\": 0, \"ENTJ\": 1, \"ESFJ\": 2, \"ENFJ\": 3, \"ISTJ\": 4, \"ISFJ\": 5, \"INTJ\": 6, \"INFJ\": 7, \"ESTP\": 8,\n",
    "          \"ESFP\": 9, \"ENTP\": 10, \"ENFP\": 11, \"ISTP\": 12, \"ISFP\": 13, \"INTP\": 14, \"INFP\": 15}\n",
    "\n",
    "# Pre-processing data\n",
    "\n",
    "df = pd.read_csv(\"16P.csv\", encoding=\"cp1252\")\n",
    "df.loc[len(df)] = df.loc[2997].copy()  # Add a random already existing row to make an even row of 60000\n",
    "df.drop(columns=[\"Response Id\"], inplace=True)\n",
    "df[\"Personality\"].replace(to_replace=p_dict, inplace=True)\n",
    "\n",
    "\n",
    "# Numpy array\n",
    "array = df.to_numpy()\n",
    "\n",
    "# Normalised array\n",
    "n_array = (array - array.min()) / (array.max() - array.min())\n",
    "\n",
    "# Split the arrays into 5 folds\n",
    "array = np.array(np.split(array, 5))\n",
    "n_array = np.array(np.split(n_array, 5))\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    k = (2 * i) + 1  # K nearest neighboorhood\n",
    "    for j in range(5):\n",
    "\n",
    "        x = array[j][:, :-1]  # Predictor array\n",
    "        x_nor = n_array[j][:, :-1]  # Normalised predictor array\n",
    "        y = array[j][:, -1]  # Target array\n",
    "\n",
    "        # Train and test arrays\n",
    "\n",
    "        idx_arr = np.array(range(5))\n",
    "        idx_arr = idx_arr[idx_arr != j]\n",
    "        # Get the tests as the current fold and the trains as the combination of other folds\n",
    "        x_train, x_test = np.concatenate(array[idx_arr], axis=0)[:, :-1], x\n",
    "        x_nor_train, x_nor_test = np.concatenate(n_array[idx_arr], axis=0)[:, :-1], x_nor\n",
    "        y_train, y_test = np.concatenate(array[idx_arr], axis=0)[:, -1], y\n",
    "\n",
    "        print(\"For k={} and fold {} as test, with feature normalization\".format(k, j + 1))\n",
    "        arr_nor = knn(x_nor_train, y_train, x_nor_test, k)\n",
    "        performance(arr_nor, y_test)\n",
    "\n",
    "        print(\"For k={} and fold {} as test, without feature normalization\".format(k, j + 1))\n",
    "        arr = knn(x_train, y_train, x_test, k)\n",
    "        performance(arr, y_test)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e6201ff",
   "metadata": {},
   "source": [
    "# ERROR ANALYSIS FOR CLASSIFICATION\n",
    "\n",
    "  For the misclassified samples that I looked through, I saw that these cases were edge cases. What I mean by that is they were also so close to another classes. They were in the edge of two different class labels and since the algorithm was not perfect, they were mistaken for the other class that it was close.\n",
    "  In my tests, when neighbor number got larger and larger, all the performance related concepts such as accuracy, precision and recall percentages went up by a little. It was most effective between k=1 and k=3. I think the reason is only looking one neighbor is more misleading than looking more. After k=3, the effect of k is much more unsignificant.\n",
    "  Normalization didn't affect the results I got by much, the reason is that all the values of the columns were between -3 and 3. Since there were little to none difference between values, scaling them between 0 and 1 affected my results only a bit.\n",
    "  The knn algorithm was very effective and the results I got from the tests were really good. Accuracy were ranging between %99.7 to %99.87 approximately. Precision were ranging between %97.62 to almost %99. Recall were ranging between %97.62 to almost %99 as well. With k being more than one, the performance and the effectiveness of the algorithm increased.\n",
    "  K-fold validation allowed me to look at the results more unbiased as I used all of the data as both test and train. It showed me the results for all of the test cases and train cases. This way the luck factor is at the lowest. With K-fold I became ensured that my algorithm was indeed successful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
